{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineResult = pickle.load(open(\"baseline_cea_T2D_JSON_version_1.1\", \"rb\"))\n",
    "nest_asyncio.apply()\n",
    "\n",
    "naiveAccuracies = []\n",
    "advancedAccuracies = []\n",
    "\n",
    "naiveF1score = []\n",
    "advancedF1score = []\n",
    "\n",
    "for tableResult in baselineResult:\n",
    "    l, e, f = tableResult\n",
    "    print(f)\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        if l[i] == \"nichts gefunden in DBlookup\":\n",
    "            l[i] = \"a/aasöldkjfnasödj\"\n",
    "    \n",
    "    allRedirectsSparqlResults = asyncio.run(get_all_wikiPageRedirects(l))\n",
    "    allRedirectsSparqlResults = np.array(allRedirectsSparqlResults)\n",
    "\n",
    "    #Sorting the responses because of asynconousity we can not asure the same order\n",
    "    sortedAllRedirectsSparqlResults = []\n",
    "    for url in l:\n",
    "        sparqlResponse = allRedirectsSparqlResults[np.where(allRedirectsSparqlResults == url)[0][0]][0]\n",
    "        sortedAllRedirectsSparqlResults.append([sparqlResponse, url])\n",
    "    \n",
    "    #creating an array with all possible redirectLinks + original url\n",
    "    #in the end: allpossibleLinksL = [[redirect1, redirect2.., originalUrl], [rdrct1, rdrct2.., oriUrl]...]\n",
    "    allpossibleLinksL = []\n",
    "    for qres, url in sortedAllRedirectsSparqlResults:\n",
    "        linksOfAResource = []\n",
    "        \n",
    "        for result in qres['results']['bindings']:\n",
    "            linksOfAResource.append(result[\"redirects\"][\"value\"])\n",
    "            \n",
    "        allpossibleLinksL.append(linksOfAResource + [url])\n",
    "\n",
    "    #compute the acccuracy manualy for the advanced method because we have to check\n",
    "    #wether or not the real url is in the redirect pool instead of naive comparison\n",
    "    accuracyHelperArray = []\n",
    "    for i in range(len(l)):\n",
    "        if e[i] in allpossibleLinksL[i]:\n",
    "            accuracyHelperArray.append(e[i])\n",
    "        else:\n",
    "            accuracyHelperArray.append(0)\n",
    "    \n",
    "    advancedAccuracies.append([accuracy_score(e, accuracyHelperArray), f])\n",
    "    naiveAccuracies.append([accuracy_score(e, l), f])\n",
    "    \n",
    "    advancedF1score.append([f1_score(e, accuracyHelperArray, average='micro'), f])\n",
    "    naiveF1score.append([f1_score(e, l,  average='micro'), f])\n",
    "\n",
    "\n",
    "#compute the overall accuracy and F1 of both methods\n",
    "advSumAcc= 0\n",
    "naiveSumAcc = 0\n",
    "advSumF1= 0\n",
    "naiveSumF1 = 0\n",
    "for i in range(len(advancedAccuracies)):\n",
    "    advSumAcc += advancedAccuracies[i][0]\n",
    "    naiveSumAcc += naiveAccuracies[i][0]\n",
    "    \n",
    "    advSumF1 += advancedF1score[i][0]\n",
    "    naiveSumF1 += naiveF1score[i][0]\n",
    "\n",
    "print(\"advAcc:\", advSumAcc/len(advancedAccuracies))\n",
    "print(\"naiveAcc:\", naiveSumAcc/len(naiveAccuracies))\n",
    "\n",
    "print(\"advf1:\", advSumF1/len(advancedF1score))\n",
    "print(\"naivef1:\", naiveSumF1/len(naiveF1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertree import *\n",
    "\n",
    "\n",
    "root = iTree(tag=\"root\")\n",
    "\n",
    "def createAllPaths(root, vectorBatches):\n",
    "    if len(vectorBatches) == 0:\n",
    "        return root\n",
    "    else:\n",
    "        #print(vectorBatches[0])\n",
    "        for label, vec in vectorBatches[0]:\n",
    "            #print(label, vec)\n",
    "            root.append(iTree(tag = label, data=str(vec)))\n",
    "        copyB = vectorBatches.copy()\n",
    "        copyB.pop(0)\n",
    "        #print(len(vectorBatches), len(vectorBatches) > 0)\n",
    "        for item in root:\n",
    "            createAllPaths(item, copyB)\n",
    "    return root\n",
    "\n",
    "tree = createAllPaths(root, allVectorBatches[0:6])\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "cosineSimilarites = []\n",
    "print(tree.tag)\n",
    "\n",
    "start = time.time()\n",
    "for c in root:\n",
    "    for cc in c:\n",
    "        for ccc in cc:\n",
    "            for cccc in ccc:\n",
    "                for ccccc in cccc:\n",
    "                    for cccccc in ccccc:\n",
    "                        vectors = [json.loads(c.d_get()), json.loads(cc.d_get()), json.loads(ccc.d_get()), json.loads(cccc.d_get()), json.loads(ccccc.d_get()), json.loads(cccccc.d_get())]\n",
    "                        #print(vectors)\n",
    "                        sim = pairwise_distances(vectors)\n",
    "                        #print(sim)\n",
    "                        print(sum(sim[np.triu_indices_from(sim, k = 1)]))\n",
    "                        #print(pairwise_distances(vectors))\n",
    "                        cosineSimilarites.append([sum(sim[np.triu_indices_from(sim, k = 1)]) ,c.tag, cc.tag, ccc.tag, cccc.tag, ccccc.tag, cccccc.tag])\n",
    "print(time.time() -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7769dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance, ratio\n",
    "from sklearn.metrics import accuracy_score, f1_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re \n",
    "import html\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, N3\n",
    "from aiosparql.client import SPARQLClient\n",
    "from treelib import Node, Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c91b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(root, vectorBatches):\n",
    "    if len(vectorBatches) != 0:\n",
    "        #print(vectorBatches[0])\n",
    "        for uri, vec in vectorBatches[0]:\n",
    "            tree.create_node(tag = uri, data = vec, parent = root.identifier)\n",
    "        copyB = vectorBatches.copy()\n",
    "        copyB.pop(0)\n",
    "        for child in tree.children(root.identifier):\n",
    "            createTree(child, copyB)\n",
    "            \n",
    "def all_paths_to_leaves(self):\n",
    "        res = []\n",
    "        \n",
    "        for leaf in self.leaves():\n",
    "            vector_path = [self.get_node(nid).data for nid in self.rsearch(leaf.identifier)][-2::-1]\n",
    "            tag_path = [self.get_node(nid).tag for nid in self.rsearch(leaf.identifier)][-2::-1]\n",
    "            res.append([vector_path, tag_path])\n",
    "\n",
    "        return res\n",
    "    \n",
    "async def get_kgvec2go_vector(candidate_Uri_pair, session):\n",
    "    #print(candidate_Uri_pair[0])\n",
    "    try:\n",
    "        async with session.get('http://kgvec2go.org/rest/get-vector/dbpedia/'+candidate_Uri_pair[0]) as res:\n",
    "            data = await res.read()\n",
    "            try:\n",
    "                jsonResp = json.loads(data)\n",
    "                return [candidate_Uri_pair[1], jsonResp[\"vector\"]]\n",
    "            except:\n",
    "                return [candidate_Uri_pair[1], [-9999]*200 ]\n",
    "    except: \n",
    "        print(\"try again\")\n",
    "        async with session.get('http://kgvec2go.org/rest/get-vector/dbpedia/'+candidate_Uri_pair[0]) as res:\n",
    "            data = await res.read()\n",
    "            try:\n",
    "                jsonResp = json.loads(data)\n",
    "                return [candidate_Uri_pair[1], jsonResp[\"vector\"]]\n",
    "            except:\n",
    "                return [candidate_Uri_pair[1], [-9999]*200 ]\n",
    "            \n",
    "        \n",
    "async def get_vectors_for_a_batch(candidateBatch):\n",
    "    tasks = []\n",
    "    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit_per_host=20)) as session:\n",
    "        for candidate_Uri_pair in candidateBatch:\n",
    "            vector_Uri_pair = asyncio.create_task(get_kgvec2go_vector(candidate_Uri_pair, session))\n",
    "            tasks.append(vector_Uri_pair)\n",
    "        all_vector_Uri_pair = await asyncio.gather(*tasks)\n",
    "    return all_vector_Uri_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2ebeca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling 1146722_1_7558140036342906956.csv 2 2 10\n",
      "handling 11688006_0_8123036130090004213.csv 3 2 10\n",
      "handling 11833461_1_3811022039809817402.csv 4 2 10\n",
      "handling 12125836_0_1134348206297032434.csv 5 2 10\n",
      "handling 12193237_0_8699643798888088574.csv 6 2 10\n",
      "handling 12271141_0_8517913935669973086.csv 7 2 10\n",
      "handling 13395751_3_369722706637560735.csv 8 2 10\n",
      "handling 14067031_0_559833072073397908.csv 9 2 10\n",
      "handling 14380604_4_3329235705746762392.csv 10 2 10\n",
      "handling 1614988_0_8789868670151796042.csv 11 2 10\n",
      "handling 16767252_0_2409448375013995751.csv 12 2 10\n",
      "handling 19073331_0_2742992342272078110.csv 13 2 10\n",
      "handling 20135078_0_7570343137119682530.csv 14 2 10\n",
      "handling 21329809_0_5526008408364682899.csv 15 2 10\n",
      "handling 21362676_0_6854186738074119688.csv 16 2 10\n",
      "handling 21585935_0_294037497010176843.csv 17 2 10\n",
      "handling 21801207_0_8144839668470123042.csv 18 2 10\n",
      "handling 24036779_0_5608105867560183058.csv 19 2 10\n",
      "handling 24142265_0_4577466141408796359.csv 20 2 10\n",
      "handling 24417511_0_3058323105166963431.csv 21 2 10\n",
      "handling 24850757_0_5978004733768297283.csv 22 2 10\n",
      "handling 25404227_0_2240631045609013057.csv 23 2 10\n",
      "handling 25933880_0_4058554985574416690.csv 24 2 10\n",
      "handling 28079336_1_3124145965038277571.csv 25 2 10\n",
      "handling 28423212_0_2742990779532526553.csv 26 2 10\n",
      "handling 29412826_0_3422707769191426540.csv 27 2 10\n",
      "handling 29414811_12_251152470253168163.csv 28 2 10\n",
      "handling 29414811_13_8724394428539174350.csv 29 2 10\n",
      "handling 29414811_2_4773219892816395776.csv 30 2 10\n",
      "handling 29414811_6_8221428333921653560.csv 31 2 10\n",
      "handling 29886325_0_1448173912684571475.csv 32 2 10\n",
      "handling 32051842_5_1757891723986774036.csv 33 2 10\n",
      "handling 33167985_0_5220049369716352813.csv 34 2 10\n",
      "handling 34041816_1_4749054164534706977.csv 35 2 10\n",
      "handling 35188621_0_6058553107571275232.csv 36 2 10\n",
      "handling 37541068_1_1156932518917623732.csv 37 2 10\n",
      "handling 37856682_0_6818907050314633217.csv 38 2 10\n",
      "handling 38428277_0_1311643810102462607.csv 39 2 10\n",
      "handling 38732532_0_1420803645394505878.csv 40 2 10\n",
      "handling 39107734_2_2329160387535788734.csv 41 2 10\n",
      "handling 3917335_0_7791699395300625164.csv 42 2 10\n",
      "handling 39173938_0_7916056990138658530.csv 43 2 10\n",
      "handling 39759273_0_1427898308030295194.csv 44 2 10\n",
      "handling 40534006_0_4617468856744635526.csv 45 2 10\n",
      "handling 41336118_0_4331895026409635103.csv 46 2 10\n",
      "handling 41819807_0_3139249984404131679.csv 47 2 10\n",
      "handling 42764224_0_515744189130384109.csv 48 2 10\n",
      "handling 43469354_0_6029017163991310319.csv 49 2 10\n",
      "handling 4444204_0_951426790527152756.csv 50 2 10\n",
      "handling 4501311_8_8306082458935575308.csv 51 2 10\n",
      "handling 45073662_0_3179937335063201739.csv 52 2 10\n",
      "handling 46646666_0_5802598112171303204.csv 53 2 10\n",
      "handling 46671561_0_6122315295162029872.csv 54 2 10\n",
      "handling 48805028_0_8933391169600128370.csv 55 2 10\n",
      "handling 48944826_0_2321751364268052533.csv 56 2 10\n",
      "handling 49772588_0_6549847739640234347.csv 57 2 10\n",
      "handling 50270082_0_444360818941411589.csv 58 2 10\n",
      "handling 51130304_0_3035822254995425429.csv 59 2 10\n",
      "handling 51741865_0_9203644246202164492.csv 60 2 10\n",
      "handling 52208141_2_5171684379987586712.csv 61 2 10\n",
      "handling 52572391_0_8684896999787304275.csv 62 2 10\n",
      "handling 53583534_0_503521622577229691.csv 63 2 10\n",
      "handling 55004961_0_2904467548072189860.csv 64 2 10\n",
      "handling 55027702_0_628532586316851176.csv 65 2 10\n",
      "handling 55238374_0_3379409961751009152.csv 66 2 10\n",
      "handling 55961337_0_6548713781034932742.csv 67 2 10\n",
      "handling 56224555_0_3713922722778385817.csv 68 2 10\n",
      "handling 57943722_0_8666078014685775876.csv 69 2 10\n",
      "handling 5873256_0_7795190905731964989.csv 70 2 10\n",
      "handling 58891288_0_1117541047012405958.csv 71 2 10\n",
      "handling 61069974_0_1894536874590565102.csv 72 2 10\n",
      "handling 61121469_0_6337620713408906340.csv 73 2 10\n",
      "handling 61753761_9_4735181228515289890.csv 74 2 10\n",
      "handling 62564020_0_3836030043284699244.csv 75 2 10\n",
      "handling 64499281_8_7181683886563136802.csv 76 2 10\n",
      "handling 66009064_0_9148652238372261251.csv 77 2 10\n",
      "handling 67865873_0_8259587610208037357.csv 78 2 10\n",
      "handling 68779923_0_3859283110041832023.csv 79 2 10\n",
      "handling 68779923_1_3240042497463101224.csv 80 2 10\n",
      "handling 68779923_2_1000046510804975562.csv 81 2 10\n",
      "handling 68779923_4_1832350971585698643.csv 82 2 10\n",
      "handling 68779923_5_2067037721454758189.csv 83 2 10\n",
      "handling 69881946_0_1105130426898457358.csv 84 2 10\n",
      "handling 70404164_0_5498195978476333642.csv 85 2 10\n",
      "handling 70883081_0_5357790421435123524.csv 86 2 10\n",
      "handling 71137051_0_8039724067857124984.csv 87 2 10\n",
      "handling 71840765_0_6664391841933033844.csv 88 2 10\n",
      "handling 72017594_0_7055198579167316152.csv 89 2 10\n",
      "handling 73242003_5_4847571983313033360.csv 90 2 10\n",
      "handling 74491133_0_7177831100884797849.csv 91 2 10\n",
      "handling 74685423_0_8061289760697103949.csv 92 2 10\n",
      "handling 74718406_0_3914457597478137388.csv 93 2 10\n",
      "handling 75367212_2_2745466355267233390.csv 94 2 10\n",
      "handling 76373906_0_6904594838446307331.csv 95 2 10\n",
      "handling 77632062_0_2292892375652659825.csv 96 2 10\n",
      "handling 77694908_0_6083291340991074532.csv 97 2 10\n",
      "handling 78650283_0_5937576517849452289.csv 98 2 10\n",
      "handling 78891639_0_3299957631631122948.csv 99 2 10\n",
      "handling 79327346_0_5787863739697336601.csv 100 2 10\n",
      "handling 80184932_0_4240003884724905487.csv 101 2 10\n",
      "handling 84548468_0_5955155464119382182.csv 102 2 10\n",
      "handling 84675086_0_6386416780680710863.csv 103 2 10\n",
      "handling 86297395_0_6919201319699354263.csv 104 2 10\n",
      "handling 90196673_0_5458330029110291950.csv 105 2 10\n",
      "handling 91474256_0_964747655387573523.csv 106 2 10\n",
      "handling 9206866_1_8114610355671172497.csv 107 2 10\n",
      "handling 93702496_0_39013305210829591.csv 108 2 10\n",
      "handling 96203994_0_2127964719640427252.csv 109 2 10\n",
      "handling 9686394_0_5980699488362107028.csv 110 2 10\n",
      "handling 97941125_0_8220652154649529701.csv 111 2 10\n",
      "handling 98929678_0_3700213490979945526.csv 112 2 10\n",
      "handling 99070098_0_2074872741302696997.csv 113 2 10\n",
      "handling 1146722_1_7558140036342906956.csv 2 2 11\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py39\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9e4dc056ba48>\u001b[0m in \u001b[0;36mget_kgvec2go_vector\u001b[1;34m(candidate_Uri_pair, session)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_kgvec2go_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_Uri_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#print(candidate_Uri_pair[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py39\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[1;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-9e4dc056ba48>\u001b[0m in \u001b[0;36mget_vectors_for_a_batch\u001b[1;34m(candidateBatch)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_Uri_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mall_vector_Uri_pair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mall_vector_Uri_pair\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py39\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m             \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-10d5db1fd2e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcandidate_chunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallCandidates_chunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mvectorBatchesTasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_vectors_for_a_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidateBatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcandidateBatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidate_chunk\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidateBatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mvector_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvectorBatchesTasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m#export all the vectors in the end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py39\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m             \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# This may also be a cancellation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "for number_of_candidates in range(2,3):\n",
    "    \n",
    "    baselineResult = pickle.load(open(\"baseline_cea_T2De_\"+str(number_of_candidates)+\"_candidates\", \"rb\"))\n",
    "    \n",
    "    for number_of_chunks in range(10,12):\n",
    "        \n",
    "        output = []\n",
    "        counter = 1\n",
    "        \n",
    "        for tableResult in baselineResult:\n",
    "            \n",
    "            labels, baselineUris, trueEntites, allCandidates, filename = tableResult\n",
    "            counter += 1\n",
    "            print(\"handling\", filename, counter, number_of_candidates, number_of_chunks)\n",
    "            \n",
    "            for index, batch in enumerate(allCandidates):\n",
    "                if not any(trueEntites[index] in x for y,x in batch):\n",
    "                    batch.append([trueEntites[index].split(\"/\")[-1], trueEntites[index]])\n",
    "\n",
    "            #split allCandidates into chunks of n=6 for less intensive similiarity measurments\n",
    "            allCandidates_chunks = [allCandidates[i:i + number_of_chunks] for i in range(0, len(allCandidates), number_of_chunks)]\n",
    "            \n",
    "            #sumC = 0\n",
    "            #for chunk in allCandidates_chunks:\n",
    "            #    sumC+= len(chunk)\n",
    "            #print(\"länge der chunks zusammen \", sumC)\n",
    "            \n",
    "            embeddingsResult = []\n",
    "            allTableVectors = []\n",
    "            for candidate_chunk in allCandidates_chunks:\n",
    "                vectorBatchesTasks = [get_vectors_for_a_batch(candidateBatch) for candidateBatch in candidate_chunk if len(candidateBatch) > 0]\n",
    "                vector_chunk = await asyncio.gather(*vectorBatchesTasks)\n",
    "                \n",
    "                #export all the vectors in the end\n",
    "                allTableVectors = allTableVectors+vector_chunk\n",
    "\n",
    "                #create a Tree to compute all the paths\n",
    "                tree = Tree()\n",
    "                root = tree.create_node(tag=\"Root\")  # root node\n",
    "                createTree(root, vector_chunk)\n",
    "\n",
    "                allPaths = all_paths_to_leaves(tree)\n",
    "\n",
    "                similiarityMetric = []\n",
    "                for vecPath, tagPath in allPaths:\n",
    "                    \n",
    "                    sim = cosine_similarity(vecPath)\n",
    "                    #sim = pairwise_distances(vecPath)\n",
    "\n",
    "                    #to exclude duplicate measurments by pairwise_distance/cosine_sim only take upper triangle from matrix results\n",
    "                    if len(sim) > 1:\n",
    "                        simSum = sum(sim[np.triu_indices_from(sim, k = 1)])\n",
    "                    else:\n",
    "                        simSum = sum(sim[0])\n",
    "                    similiarityMetric.append([simSum, tagPath])\n",
    "                    \n",
    "                #pick the URIpath that has most similarites between them\n",
    "                #min or max depending on distance or similarity\n",
    "                embeddingsResult = embeddingsResult+(max(similiarityMetric, key= lambda k: k[0])[1])\n",
    "            output.append([labels, embeddingsResult, trueEntites, allTableVectors, filename])\n",
    "        pickle.dump(output, open( \"embeddings_cea_T2De_\"+str(number_of_candidates)+\"_candidates_\"+str(number_of_chunks)+\"_chunks_cosineSimilarity_added_trueEntities\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
